---
title: "Adjusted Limited Dependent Variable Mixture Models of Health State Utilities in R"
author: Mark Pletscher
email: <pletscher.mark@gmail.com>
date: "`r Sys.Date()`"
package: aldvmm
output: 
  bookdown::pdf_document2:
    toc: false
bibliography: aldvmm_bib.bib
vignette: >
  %\VignetteIndexEntry{Adjusted Limited Dependent Variable Mixture Models of Health State Utilities in R}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<!-- Setup -->

```{r setup, hide, eval = TRUE, echo = FALSE, results = 'hide'}

# Load packages
#--------------

library("aldvmm")
library("xtable")
library("ggplot2")
library("scales")

# Custom ggplot theme
#--------------------

ggplot_theme <- theme(panel.background = element_rect(fill = "white",
                                                      colour = "white",
                                                      size = 0.5, 
                                                      linetype = "solid"),
                      panel.grid.major = element_blank(), 
                      panel.grid.minor = element_blank(),
                      panel.grid.major.y = element_line(size = 0.25, 
                                                        linetype = 'dashed', 
                                                        colour = "grey"),
                      text = element_text(size = 11),
                      plot.margin = margin(10, 10, 0, 10),
                      axis.line = element_line(size = 0.25),
                      axis.text = element_text(size = 11),
                      #legend.title=element_text(size = 11),
                      legend.title = element_blank(),
                      legend.text=element_text(size = 11),
                      legend.position = 'bottom',
                      legend.direction = "vertical",
                      legend.key = element_blank())

```

<!-- Computationally heavy calculations -->

```{r calc, hide, eval = FALSE, echo = FALSE, results = 'hide'}

#------------------------------------------------------------------------------
# Helper functions
#------------------------------------------------------------------------------

# Summary table print function
#-----------------------------

reptab.fit <- function(fit) {
  
  tmp <- summary(fit)
  
  valindex <- apply(tmp, 1, function(x)
    sum("-" != strsplit(paste(x, collapse = ""), "")[[1]])!=0
  )
  tmp <- tmp[valindex, ]
  
  colnames(tmp) <- tmp[1, ]
  tmp <- tmp[-1, ]
  #tmp[nrow(tmp), 3] <- ""
  #tmp[nrow(tmp), 4] <- ""
  
  eindex <- match("E[y|c, X]", tmp[, 1])
  pindex <- match("P[c|X]", tmp[, 1])
  
  lindex <- c(-1, eindex - 1, eindex, pindex - 1, pindex, nrow(tmp) - 1)
  
  list(table = tmp,
       lindex = lindex[!is.na(lindex)])
}

# Stata table print function
#---------------------------

reptab.stata <- function(file) {
  
  stata <- read.table(file, 
                      header = FALSE, 
                      sep = ";", 
                      dec = ".")
  
  stata[1, 1] <- ""
  names(stata) <- stata[1, ]
  stata <- stata[-1, ]
  
  eindex <- match("E[y|c, X]", stata[, 1])
  pindex <- match("P[c|X]", stata[, 1])
  
  lindex <- c(-1, eindex - 1, eindex, pindex - 1, pindex, nrow(stata) - 1)
  
  list(table = stata,
       lindex = lindex[!is.na(lindex)])
}

# ggplot theme
#-------------

ggplot_theme <- theme(panel.background = element_rect(fill = "white",
                                                      colour = "white",
                                                      size = 0.5, 
                                                      linetype = "solid"),
                      panel.grid.major = element_blank(), 
                      panel.grid.minor = element_blank(),
                      panel.grid.major.y = element_line(size = 0.25, 
                                                        linetype = 'dashed', 
                                                        colour = "grey"),
                      text = element_text(size = 11),
                      plot.margin = margin(10, 10, 0, 10),
                      axis.line = element_line(size = 0.25),
                      axis.text = element_text(size = 11),
                      #legend.title=element_text(size = 11),
                      legend.title = element_blank(),
                      legend.text=element_text(size = 11),
                      legend.position = 'bottom',
                      legend.direction = "vertical",
                      legend.key = element_blank())

# Modified Hosmer-Lemeshow test function
#---------------------------------------

est_mhl <- function(fit, ngroup) {
  
  # Select sample
  yhat <- fit[["pred"]][["yhat"]]
  res <- fit[["pred"]][["res"]]
  
  nsample <- length(yhat)
  sample <- sample(1:length(yhat), nsample, replace = FALSE)
  
  yhat <- yhat[sample]
  res <- res[sample]
  
  # Make groups
  group <- as.numeric(cut(yhat, breaks = ngroup), na.rm=TRUE)
  
  # Auxiliary regression
  fit <- lm(res ~ factor(group))
  
  # Data set of predictions from auxiliary regressions
  tmpdat <- data.frame(group = unique(group)[order(unique(group))])
  predict <- predict(fit, 
                     newdata = tmpdat, 
                     se.fit = TRUE, 
                     interval = 'confidence', 
                     level = 0.95)
  
  plotdat <- as.data.frame(rbind(
    cbind(group = tmpdat$group, 
          outcome = "mean",
          value = predict$fit[ , 'fit']),
    cbind(group = tmpdat$group, 
          outcome = "ll",
          value = predict$fit[ , 'lwr']),
    cbind(group = tmpdat$group, 
          outcome = "ul",
          value = predict$fit[ , 'upr'])
  ))
  
  
  # Make plot
  lab <- paste0(seq(from = 1/ngroup, to = 1, by = 1/ngroup)*100, '%')
  
  plot <- ggplot2::ggplot(plotdat, 
                          aes(x = factor(as.numeric(group)), 
                              y = as.numeric(value), 
                              group = factor(outcome))) +
    geom_line(aes(linetype = factor(outcome))) +
    geom_hline(yintercept = 0) +
    scale_linetype_manual(values=c("dashed", "solid", "dashed")) +
    scale_x_discrete(breaks = 1:ngroup, labels = lab) +
    coord_cartesian(ylim = c(-0.2, 0.2)) +
    xlab("Precentiles of expected values") +
    ylab("Mean residuals") +
    ggplot_theme +
    theme(legend.position = "none")
  
  return(plot)
  
}

#------------------------------------------------------------------------------
# Download data
#------------------------------------------------------------------------------

temp <- tempfile()
download.file(paste0("https://files.digital.nhs.uk/publicationimport",
                     "/pub11xxx/pub11359/final-proms-eng-apr11-mar12",
                     "-data-pack-csv.zip"), temp)
df <- read.table(unz(description = temp,
                     filename = 'Hip Replacement 1112.csv'),
                 sep = ',',
                 header = TRUE)
unlink(temp)
rm(temp)

df <- df[df$AGEBAND!='*' & df$SEX!='*', c('AGEBAND', 'SEX', 'Q2_EQ5D_INDEX',
                                          'HR_Q2_SCORE')]

df$eq5d <- df$Q2_EQ5D_INDEX
df$hr <- df$HR_Q2_SCORE/10

df <- df[stats::complete.cases(df), ]

set.seed(101010101)
df <- df[sample(1:nrow(df), size = nrow(df)*0.3), ]

#------------------------------------------------------------------------------
# Fit model 1
#------------------------------------------------------------------------------

formula <- eq5d ~ hr | 1

# Assess all optimization methods
#--------------------------------

init.method <- c("zero", "random", "constant", "sann")

optim.method <- c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B", 
                  #"nlm", 
                  "nlminb", 
                  "Rcgmin", "Rvmmin","hjn")

fit1_all <- list()

for (i in init.method) {
  for (j in optim.method) {
    
    cat(paste0("\n", i, " - ", j, "\n"))
    
    start <- Sys.time()
    
    set.seed(101010101)
    fit <- tryCatch({
      aldvmm::aldvmm(data = df,
                     formula = formula,
                     psi = c(0.883, -0.594),
                     ncmp = 2,
                     init.method = i,
                     optim.method = j)
      
    }, error = function(e) {
      return(list())
    })
    
    end <- Sys.time()
    
    fit1_all[["fit"]][[i]][[j]] <- fit
    fit1_all[["time"]][[i]][[j]] <- difftime(end, start, units = c("mins"))
    rm(fit, start, end)
    
  }
}

save(fit1_all, 
     file = "fit1_all.RData",
     compress = TRUE)

rm(fit1_all, init.method, optim.method, i, j)

# Constrained optimization
#-------------------------

# c(0.2358245, 0.1458986, -0.4306492, 0.3134739, 0.7282714, -2.4622704, -1.2480114) 

init <- c(0,    0,   0,   0,    0,    0,    0.7283)
lo   <- c(-Inf, -Inf, -3,  -Inf, -Inf, -3, -Inf)
hi   <- c(Inf,  Inf,  Inf, Inf,  Inf,  Inf,  Inf)

fit1_cstr <- aldvmm::aldvmm(data = df,
                            formula = formula,
                            psi = c(0.883, -0.594),
                            ncmp = 2,
                            init.est = init,
                            init.lo = lo,
                            init.hi = hi)

save(fit1_cstr, 
     file = "fit1_cstr.RData",
     compress = TRUE)

rm(fit1_cstr, init, lo, hi)

# Constrained optimization Hernandez Alava and Wailoo (2015)
#-----------------------------------------------------------

init <- c(-.0883435, .2307964, 100, 0, 7.320611, -1.646109, 1.00e-30)
lo <- c(-Inf, -Inf, 100, -Inf, -Inf, -Inf, 1e-30)
hi <- c(Inf,   Inf, Inf,    0,  Inf,  Inf, Inf)

fit1_cstr_stata <- aldvmm::aldvmm(data = df,
                                  formula = formula,
                                  psi = c(0.883, -0.594),
                                  ncmp = 2,
                                  init.est = init,
                                  init.lo = lo,
                                  init.hi = hi)

save(fit1_cstr_stata, 
     file = "fit1_cstr_stata.RData",
     compress = TRUE)

rm(fit1_cstr_stata, init, lo, hi)

# Single-component model
#-----------------------

fit1_tobit <- aldvmm::aldvmm(data = df,
                             formula = formula,
                             psi = c(0.883, -0.594),
                             ncmp = 1,
                             optim.method = "hjn")

save(fit1_tobit, 
     file = "fit1_tobit.RData",
     compress = TRUE)

rm(fit1_tobit)
rm(formula)

#------------------------------------------------------------------------------
# Fit model 2
#------------------------------------------------------------------------------

formula <- eq5d ~ hr | hr

# Zero starting values
#---------------------

fit2 <- aldvmm::aldvmm(data = df,
                       formula = formula,
                       psi = c(0.883, -0.594),
                       ncmp = 2,
                       optim.method = "hjn")

save(fit2, 
     file = "fit2.RData",
     compress = TRUE)

rm(fit2)

# Starting values from Hernandez Alava and Wailoo (2015)
#-------------------------------------------------------

init <- c(-.40293118, .30502755, .22614716, .14801581, -.70755741, 0, 
          -1.2632051, -2.4541401)

fit2_stata <- aldvmm::aldvmm(data = df,
                             formula = formula,
                             psi = c(0.883, -0.594),
                             ncmp = 2,
                             init.est = init,
                             optim.method = "hjn")

save(fit2_stata, 
     file = "fit2_stata.RData",
     compress = TRUE)

rm(fit2_stata, init)

rm(formula)

#------------------------------------------------------------------------------
# Plots
#------------------------------------------------------------------------------

# Histogram of observed outcomes
#-------------------------------

plot_hist_obs <- ggplot2::ggplot(df, aes(x = eq5d)) + 
  geom_histogram(binwidth = 0.02, color="black", fill="white") +
  xlab("EQ-5D-3L utilities") +
  ylab("Frequency") +
  scale_y_continuous(labels = scales::comma) +
  ggplot_theme

save(plot_hist_obs, 
     file = "plot_hist_obs.RData",
     compress = TRUE)

rm(plot_hist_obs)

# Histogram of predicted outcomes Model 1, zero starting values, Nelder-Mead
#---------------------------------------------------------------------------

load("fit1_all.RData")

yhat <- fit1_all[["fit"]][["zero"]][["Nelder-Mead"]][["pred"]][["yhat"]]

plot_hist_pred <- ggplot2::ggplot(as.data.frame(yhat), aes(x = yhat)) + 
  geom_histogram(binwidth = 0.02, color="black", fill="white") +
  xlab("EQ-5D-3L utilities") +
  ylab("Frequency") +
  scale_y_continuous(labels = scales::comma) +
  ggplot_theme

save(plot_hist_pred, 
     file = "plot_hist_pred.RData",
     compress = TRUE)

rm(yhat, plot_hist_pred, fit1_all)

# Comparison of predicted values from different optimization methods
#-------------------------------------------------------------------

load("fit1_all.RData")

a <- fit1_all[["fit"]][["zero"]][["Nelder-Mead"]][["pred"]][["yhat"]]
b <- fit1_all[["fit"]][["zero"]][["BFGS"]][["pred"]][["yhat"]]
c <- fit1_all[["fit"]][["zero"]][["hjn"]][["pred"]][["yhat"]]

tmpdf <- as.data.frame(rbind(cbind(y = c, bl = c, alg = "45Â° line"),
                             cbind(y = a, bl = c, alg = "Nelder-Mead"),
                             cbind(y = b, bl = c, alg = "BFGS")))

tmpdf$y <- as.numeric(tmpdf$y)
tmpdf$bl <- as.numeric(tmpdf$bl)

plot_comp_pred <- ggplot(tmpdf, aes(x = bl, y = y, group = alg)) +
  geom_line(aes(linetype = alg)) +
  scale_linetype_manual(values=c("solid", "dashed", "dotted")) +
  scale_x_continuous(breaks = seq(-0.2, 1, by = 0.1)) +
  xlab("E[y|X] Hooke and Jeeves Pattern Search Optimization (hjn)") +
  ylab("E[y|X]") +
  guides(linetype = guide_legend(nrow = 1)) +
  ggplot_theme +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_blank())

save(plot_comp_pred, 
     file = "plot_comp_pred.RData",
     compress = TRUE)

rm(fit1_all, a, b, c, tmpdf, plot_comp_pred)

# Modified Hosmer-Lemeshow test by optimization method
#-----------------------------------------------------

load("fit1_all.RData")

set.seed(101010101)
plot_comp_mhl1 <- est_mhl(fit1_all[["fit"]][["zero"]][["Nelder-Mead"]],
                          ngroup = 10)
save(plot_comp_mhl1, 
     file = "plot_comp_mhl1.RData",
     compress = TRUE)
rm(plot_comp_mhl1)

set.seed(101010101)
plot_comp_mhl2 <- est_mhl(fit1_all[["fit"]][["zero"]][["BFGS"]],
                          ngroup = 10)
save(plot_comp_mhl2, 
     file = "plot_comp_mhl2.RData",
     compress = TRUE)
rm(plot_comp_mhl2)

set.seed(101010101)
plot_comp_mhl3 <- est_mhl(fit1_all[["fit"]][["zero"]][["hjn"]],
                          ngroup = 10)
save(plot_comp_mhl3,
     file = "plot_comp_mhl3.RData",
     compress = TRUE)
rm(plot_comp_mhl3)

rm(fit1_all)

#------------------------------------------------------------------------------
# Tables
#------------------------------------------------------------------------------

# Model 1, comparison of optimization methods
#--------------------------------------------

load("fit1_all.RData")

tab_comp_ll <- matrix(NA,
                      nrow     = length(fit1_all[["fit"]]),
                      ncol     = length(fit1_all[["fit"]][[1]]),
                      dimnames = list(names(fit1_all[["fit"]]),
                                      names(fit1_all[["fit"]][[1]])))
tab_comp_time <- tab_comp_ll
tab_comp_cov <- tab_comp_ll
tab_comp_mse <- tab_comp_ll
tab_comp_mae <- tab_comp_ll

for (i in names(fit1_all[["fit"]])) { # Initial value methods
  for (j in names(fit1_all[["fit"]][[1]])) { # Optimization methods
    if (length(fit1_all[["fit"]][[i]][[j]])!=0) {
      tab_comp_ll[i, j] <- -fit1_all[["fit"]][[i]][[j]][["gof"]][["ll"]]
      tab_comp_time[i, j] <- fit1_all[["time"]][[i]][[j]]
      cov <- fit1_all[["fit"]][[i]][[j]][["cov"]]
      tab_comp_cov[i, j] <- sum(diag(cov)<0)==0 & sum(is.na(diag(cov)))==0
      rm(cov)
      tab_comp_mse[i, j] <- fit1_all[["fit"]][[i]][[j]][["gof"]][["mse"]]
      tab_comp_mae[i, j] <- fit1_all[["fit"]][[i]][[j]][["gof"]][["mae"]]
    }
  }
}

save(tab_comp_ll, 
     file = "tab_comp_ll.RData",
     compress = TRUE)

save(tab_comp_time, 
     file = "tab_comp_time.RData",
     compress = TRUE)

save(tab_comp_cov, 
     file = "tab_comp_cov.RData",
     compress = TRUE)

save(tab_comp_mse, 
     file = "tab_comp_mse.RData",
     compress = TRUE)

save(tab_comp_mae, 
     file = "tab_comp_mae.RData",
     compress = TRUE)

rm(tab_comp_ll, tab_comp_time, tab_comp_cov, tab_comp_mse, tab_comp_mae, i, j)
rm(fit1_all)

# Model 1 comparison of coefficients by optimization method
#----------------------------------------------------------

load("fit1_all.RData")

tmplist1 <- reptab.fit(fit1_all[["fit"]][["zero"]][["Nelder-Mead"]])
tmplist2 <- reptab.fit(fit1_all[["fit"]][["zero"]][["BFGS"]])
tmplist3 <- reptab.fit(fit1_all[["fit"]][["zero"]][["hjn"]])

tab_comp_coef <- cbind(tmplist1$table[, 1:3], 
                       tmplist2$table[, 3], 
                       tmplist3$table[, 3]) 

names(tab_comp_coef)  <- c("", "", "Nelder-Mead", "BFGS", "hjn")

save(tab_comp_coef, 
     file = "tab_comp_coef.RData",
     compress = TRUE)

rm(tmplist1, tmplist2, tmplist3, tab_comp_coef)
rm(fit1_all)

# Summary table model 1, zero, Nelder-Mead 
#-----------------------------------------

load("fit1_all.RData")

tab_sum_mod1 <- reptab.fit(fit1_all[["fit"]][["zero"]][["Nelder-Mead"]])

save(tab_sum_mod1, 
     file = "tab_sum_mod1.RData",
     compress = TRUE)

rm(tab_sum_mod1)
rm(fit1_all)

# Summary table model 1, zero, BFGS 
#----------------------------------

load("fit1_all.RData")

tab_sum_mod1bfgs <- reptab.fit(fit1_all[["fit"]][["zero"]][["BFGS"]])

save(tab_sum_mod1bfgs, 
     file = "tab_sum_mod1bfgs.RData",
     compress = TRUE)

rm(tab_sum_mod1bfgs)
rm(fit1_all)

# Summary table model 1, constant-only model as starting values, BFGS 
#--------------------------------------------------------------------

load("fit1_all.RData")

tab_sum_const <- reptab.fit(fit1_all[["fit"]][["constant"]][["BFGS"]])

save(tab_sum_const, 
     file = "tab_sum_const.RData",
     compress = TRUE)

rm(tab_sum_const)
rm(fit1_all)

# Summary table model 1, zero, constrained 
#-----------------------------------------

load("fit1_cstr.RData")

tab_sum_cstr <- reptab.fit(fit1_cstr)

save(tab_sum_cstr, 
     file = "tab_sum_cstr.RData",
     compress = TRUE)

rm(fit1_cstr, tab_sum_cstr)

# Summary table model 1, constrained with stata estimates as starting values 
#---------------------------------------------------------------------------

load("fit1_cstr_stata.RData")

tab_sum_cstata <- reptab.fit(fit1_cstr_stata)

save(tab_sum_cstata, 
     file = "tab_sum_cstata.RData",
     compress = TRUE)

rm(fit1_cstr_stata, tab_sum_cstata)

# Summary table model 1, single-component, zero starting values, hjn 
#-------------------------------------------------------------------

load("fit1_tobit.RData")

tab_sum_tobit <- reptab.fit(fit1_tobit)

save(tab_sum_tobit, 
     file = "tab_sum_tobit.RData",
     compress = TRUE)

rm(fit1_tobit, tab_sum_tobit)

# Summary table model 2
#----------------------

load("fit2.RData")

tab_sum_mod2 <- reptab.fit(fit2)

save(tab_sum_mod2, 
     file = "tab_sum_mod2.RData",
     compress = TRUE)

rm(fit2, tab_sum_mod2)

# Summary table model 2, stata estimates as starting values
#----------------------------------------------------------

load("fit2_stata.RData")

tab_sum_mod2stata <- reptab.fit(fit2_stata)

save(tab_sum_mod2stata, 
     file = "tab_sum_mod2stata.RData",
     compress = TRUE)

rm(fit2_stata, tab_sum_mod2stata)

# Stata 1 summary table
#----------------------

tab_sum_stata1 <- reptab.stata("hernandez_2015_model1_default.csv")

save(tab_sum_stata1, 
     file = "tab_sum_stata1.RData",
     compress = TRUE)

rm(tab_sum_stata1)

# Stata 2 summary table
#----------------------

tab_sum_stata2 <- reptab.stata("hernandez_2015_model1_cons.csv")

save(tab_sum_stata2, 
     file = "tab_sum_stata2.RData",
     compress = TRUE)

rm(tab_sum_stata2)

# Stata 3 summary table
#----------------------

tab_sum_stata3 <- reptab.stata("hernandez_2015_model2.csv")

save(tab_sum_stata3, 
     file = "tab_sum_stata3.RData",
     compress = TRUE)

rm(tab_sum_stata3)

rm(list = ls(pattern = "fit"))
rm(list = ls(pattern = "reptab"))
rm(ggplot_theme, df, est_mhl)

```  

# Introduction

Health-related quality of life is a key outcome in health technology assessments because it is patient-relevant and it is needed to calculate quality adjusted life years in economic evaluations of health care interventions. Quality of life instruments usually measure health problems in multiple domains using ordinal Likert scales. Value sets or valuation functions are then used to convert these profiles of ordinal measures into cardinal health-related utilities between 1 (perfect health) and minus infinity, where 0 represents death and negative values represent health states worse than death. Because perfect health is defined as 100% quality of life, health state utilities are limited at 1. The lowest possible utility in a local value set or valuation function further defines a lower limit of health state utilities in a local population. Thus, health state utilities are limited dependent variables. In addition, health state utilities often show gaps between 1 and the next smaller utility in a value set or valuation function. These gaps occur particularly often in  quality of life instruments with few levels in the ordinal measures of health problems such as the EQ-5D-3L [@Mulhern2018]. A last but important particularity of health state utilities is that they can be the consequence of multiple latent classes, or they can exhibit multi-modal marginal densities [@HernandezAlava2014].

Adjusted limited dependent variable mixture models are finite mixtures of multiple normal distributions that account for multi-modality or latent classes, limits and gaps between 1 and the next smaller utility value [@HernandezAlava2012; @HernandezAlava2013; @HernandezAlava2014; @HernandezAlava2015; @Mukuria2019]. These features can improve empirical fit, parameter identification and predictive accuracy compared to standard regression models which makes adjusted limited dependent variable mixture models particularly useful for mapping studies [@Gray2018a; @Gray2018; @Dixon2020; @Yang2019; @Xu2020; @Fuller2017; @Pennington2020] and the identification of incremental effects and average marginal effects of medical interventions or health problems (e.g. @Hvidberg2016).

The R package 'aldvmm' is an implementation of the adjusted limited dependent variable model mixture model proposed by @HernandezAlava2015 using normal component distributions and a multinomial logit model of probabilities of component membership.

The objective of this vignette is to demonstrate the usage of the 'aldvmm' package, show important challenges of fitting adjusted limited dependent variable mixture models and to compare its results to the published results from the STATA\textsuperscript{\textregistered} package [@HernandezAlava2015] using publicly available data.

# Methods

Adjusted limited dependent variable mixture models are finite mixtures of normal distributions in $K$ components $c$ with conditional expectations $E[y|X, c] = X\beta^{c}$ and standard deviations $\sigma^{c}$. The probabilities of component membership are estimated using a multinomial logit model as $P[c|X]=exp(X\delta^{c})/\sum_{k=1}^{K}exp(X\delta^{k})$. The model accumulates the density mass of the finite mixture below a minimum value $\Psi_1$ at the value $\Psi_1$, and the density mass above a maximum value $\Psi_{2}$ at 1. If the maximum value $\Psi_2$ is smaller than 1, the model emulates a value set with a gap between 1 and the next smaller value.

\begin{equation}
\label{eq:limits}
\begin{array}{ll}
y_{i}|c =& \begin{cases} \begin{array}{ll}
1        & \text{if } y_{i}|c > \Psi_{2}\\
\Psi_{1} & \text{if } y_{i}|c \leq \Psi_{1}\\
y_{i}|c  & \text{if } \Psi_{1} < y_{i}|c \leq \Psi_{2}\\
\end{array} \end{cases}
\end{array}
\end{equation}

In this vignette, we estimate the same models of post-operative EQ-5D-3L utilities as @HernandezAlava2015 and include post-operative Oxford Hip Scores (divided by 10) as the only explanatory variable $x$.

\begin{equation}
\label{eq:models}
\begin{array}{lrl}
\text{Model 1:}& E[y|c, X] &= \beta_{0}^{c} + \beta_{1}^{c}x\\
& P[c|X]    &= \text{mlogit}(\delta_{0}^{c})\\
&&\\
\text{Model 2:}& E[y|c, X] &= \beta_{0}^{c}  + \beta_{1}^{c}x\\
& P[c|X]    &= \text{mlogit}(\delta_{0}^{c} + \delta_{1}^{c}x)
\end{array}
\end{equation}

The function aldvmm::aldvmm() fits an adjusted limited dependent variable mixture model using the likelihood function from @HernandezAlava2015. The function calls optimr::optimr to minimize the negative log-likelihood function using numerical gradients from numDeriv::grad(). The function aldvmm::aldvmm() accepts all optimization methods available in optimr::optimr() except for "nlm", which requires a different implementation of the likelihood function.

The model formula in aldvmm::aldvmm() is a object of class "formula" with two parts on the right-hand side. The first part on the left of the | delimiter represents the model of expected values of $K$ normal distributions in components $c$ with standard deviations $\sigma^{c}$. The second part on the right of the | delimiter represents the model of probabilities of component membership from a multinomial logit model.

The function aldvmm::aldvmm() provides four options for the generation of starting values for the optimization algorithm. 

1. "zero": A vector of zeroes (default). 

2. "random": A vector of standard normal random values.

3. "constant": Parameter estimates of a constant-only model as starting values for intercepts and standard deviations, and zeroes for all other parameters.\footnote{The auxiliary models for obtaining starting values are fitted using zero starting values.}

4. "sann": Parameter estimates of a simulated annealing algorithm.

The package 'aldvmm' obtains fitted values using the expected value function from @HernandezAlava2015. Covariance matrices and standard errors of parameters are obtained from a numerical approximation of the hessian matrix using numDeriv::hessian(). Standard errors of fitted values in the estimation data $SE_{fit}$ and standard errors of predicted values in new data  $SE_{pred}$ are calculated using the delta method [@Dowd2014; @Whitmore1986]. $G$ denotes the gradient of fitted values with respect to changes in parameter estimates, $\Sigma$ denotes the covariance matrix of parameters, and $MSE$ denotes the mean squared error of fitted versus observed values in the estimation data.
\begin{equation}
\begin{array}{rl}
SE_{fit} &= \sqrt{G'\Sigma G}
\end{array}
\end{equation}
\begin{equation}
\begin{array}{rl}
SE_{pred} &= \sqrt{MSE + G'\Sigma G}
\end{array}
\end{equation}

The function aldvmm::aldvmm() returns an object of S3 class "aldvmm" for which methods for generic functions summary() and predict() are available.

# Installation

The package 'aldvmm' can be installed from github using the function install_github() from the 'devtools' package.

```{r install, eval = FALSE, echo = TRUE}
# install.packages("devtools")
devtools::install_github("pletschm/aldvmm", ref = "main")
```

# Data

We analyze the same publicly available [$\text{\underline{EQ-5D-3L utility data}}$](https://digital.nhs.uk/data-and-information/publications/statistical/patient-reported-outcome-measures-proms/finalised-patient-reported-outcome-measures-proms-in-england-april-2011-to-march-2012) from English patients after hip replacement in 2011 and 2012 [@NHSDigital2013] as @HernandezAlava2015 in their description of the STATA\textsuperscript{\textregistered} ALDVMM package.

```{r data, eval = FALSE, echo = TRUE}
temp <- tempfile()

url <- paste0("https://files.digital.nhs.uk/publicationimport/pub11xxx/",
              "pub11359/final-proms-eng-apr11-mar12-data-pack-csv.zip")

download.file(url, temp)

df <- read.table(unz(description = temp,
                     filename = 'Hip Replacement 1112.csv'),
                 sep = ',',
                 header = TRUE)

unlink(temp)
rm(temp)

df <- df[, c("AGEBAND", "SEX", "eq5d", "HR_Q2_SCORE")]
df <- df[df$AGEBAND!='*' & df$SEX!='*', ]
d[, "hr"] <- d[, "HR_Q2_SCORE"]/10
df <- df[stats::complete.cases(df), ]

set.seed(101010101)
df <- df[sample(1:nrow(df), size = nrow(df)*0.3), ]
```

The data includes 35'166 observations with complete information on patients' post-operative utilities, Oxford Hip Scores, age and sex. Like @HernandezAlava2015, we draw a 30% sub-sample of 10'549 observations from the population of complete observations of these variables. Although we follow the same approach as  @HernandezAlava2015 in the preparation of the data, our random sample is not identical to the data used in their study. Post-operative EQ-5D-3L utilities from English value sets [@Dolan1997] show a bimodal distribution, limits at -0.594 and 1 and a gap between 1 and 0.883 (figure \@ref(fig:plot-hist-obs)).

```{r plot-hist-obs, echo = FALSE, results = 'asis', fig.height = 2.9, fig.width = 6, fig.cap = "Frequency distribtution of observed EQ-5D-3L utilities"}

load("plot_hist_obs.RData")

plot_hist_obs

rm(plot_hist_obs)

```

# Results

## Model 1

### BFGS optimization method with zero-only starting values {#sec:base}

We first fit model 1 with the "BFGS" optimization method and "zero" initial values. The values in the argument 'psi' represent the maximum and minimum values smaller than 1 in the English value set [@Dolan1997]. As the data showed a bi-modal distribution (figure \@ref(fig:plot-hist-obs)), we estimate a mixture of 2 normal distributions ('ncmp' = 2). aldvmm::aldvmm() returns an object of class "aldvmm".

```{r model1-fit, echo = TRUE, eval = FALSE}
library("aldvmm")

fit <- aldvmm::aldvmm(data = df,
                      formula = eq5d ~ hr | 1,
                      psi = c(0.883, -0.594),
                      ncmp = 2,
                      init.method = "zero",
                      optim.method = "BFGS")

summary(fit)

pred <- predict(fit,
                newdata = df,
                se.fit = TRUE,
                type = "fit")
```

We obtain a summary table of regression results using the generic function summary(). The model converges at a log-likelihood of 706.32 and an Akaike information criterion value of -1'398.65 (table \@ref(tab:tab-sum-mod1bfgs)).

The coefficients of the intercept and covariates for the expected values $E[y|c, X]$ of the normal distributions can be interpreted as marginal effects on component means. 'lnsigma' denotes the natural logarithm of the estimated standard deviation $\sigma^{c}$. The coefficients of covariates in the multinomial logit model of probabilities of component membership are log-transformed relative probabilities. Our model only includes two components, and the multinomial logit model collapses to a binomial logit model. The intercept of 0.7283 means that the average probability of an observation in the data to belong to the first component is $\text{exp}(0.7283)$ or 2.071556 times the probability to belong to component 2.

```{r tab-sum-mod1, echo = FALSE, results = "asis"}

load("tab_sum_mod1bfgs.RData")

print(xtable::xtable(tab_sum_mod1bfgs$table, 
                     align = "lllrrrrrr",
                     label = "tab:tab-sum-mod1bfgs",
                     caption = 'Regression results from model1 with "BFGS" optimization method and "zero" starting values'),
      type = "latex",
      include.rownames = FALSE,
      hline.after = tab_sum_mod1bfgs$lindex,
      comment = FALSE,
      caption.placement = "top")

rm(tab_sum_mod1bfgs)

```

We obtain expected values of observations in the estimation data using the generic function predict(). Standard errors of fitted (estimation data) or predicted (new data) values are calculated using the delta method. Expected values exhibit a smoother distribution than observed values and do not show a gap between 1 and 0.883, because they are averages of 1 and the expected values of the finite mixture weighted by the density mass above and below 0.883.

```{r plot-hist-pred, echo = FALSE, results = "asis", fig.height = 2.9, fig.width = 6,  fig.cap = "Expected values from base case model"}

load("plot_hist_pred.RData")

plot_hist_pred

rm(plot_hist_pred)

```

### Comparison of optimization methods

```{r fit-comp, echo = TRUE, eval = FALSE}
init.method <- c("zero", "random", "constant", "sann")

optim.method <- c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "nlminb", "Rcgmin", 
                  "Rvmmin","hjn")

fit1_all <- list()

for (i in init.method) {
  for (j in optim.method) {
    set.seed(101010101) # Seed for random starting values
    fit1_all[[i]][[j]] <- aldvmm::aldvmm(data         = df,
                                         formula      = formula,
                                         psi          = c(0.883, -0.594),
                                         ncmp         = 2,
                                         init.method  = i,
                                         optim.method = j)
  }
}
```

@HernandezAlava2015 suggested that the likelihood function of the adjusted limited dependent variable mixture model with the English EQ-5D-3L data might have multiple local optima, and that the estimation is sensitive to initial values. We thus fit model 1 with all optimization algorithms and methods for generating initial values available in aldvmm::aldvmm() to assess the sensitivity of the model to optimization settings and to find the maximum likelihood estimates.

The maximum likelihood varies considerably across optimization methods and initial values which confirms the sensitivity of the model to initial values and optimzation algorithms (table \@ref(tab:ll)). The most frequent log-likelihood is 706.32, but the Hooke and Jeeves Pattern Search Optimization ("hjn") with "zero" initial values converges at a log-likelihood 33'057.43.

The optimization methods "Nelder-Mead", "BFGS", "CG", and "L-BFGS-B" are particularly sensitive to starting values. The methods "Rvmmin" coverge at a log-likelihood of 706.32 with three of four sets of initial values, and the method "hjn" with two of four sets of initial values. The methods "nlminb" and "Rcgmin" converge at a log-likelihood of 706.32 regardless of the sets of initial values.

```{r tab-comp-ll, echo = FALSE, results = 'asis'}

load("tab_comp_ll.RData")

print(xtable::xtable(tab_comp_ll, 
                     align = paste0("l", 
                                    paste(rep("r", ncol(tab_comp_ll)), 
                                          collapse = "")),
                     label = "tab:ll",
                     caption = "Log-likelihood by optimization method"),
      type = "latex",
      include.rownames = TRUE,
      hline.after = c(-1, 0, nrow(tab_comp_ll)),
      comment = FALSE,
      caption.placement = "top")

rm(tab_comp_ll)

```

The computation times of optimization routines vary considerably across methods (table \@ref(tab:time)). The optimization methods "Nelder-Mead", "BFGS", "L-BFGS-B" and "Rvmmin" are the fastest methods, but this speed comes at the cost of a higher risk of convergence at local optima. Naturally, the generation of initial values using simulated annealing ("sann") is the slowest method for generating initial values which results in long overall computation times of aldvmm::aldvmm(). The Hooke and Jeeves Pattern Search Optimization ("hjn") with "zero" starting values that converges at the largest log-likelihood is the slowest approach with a computation time of 15.83 minutes.

```{r tab-comp-time, echo = FALSE, results = 'asis'}

load("tab_comp_time.RData")

print(xtable::xtable(tab_comp_time, 
                     align = paste0("l", 
                                    paste(rep("r", ncol(tab_comp_time)), 
                                          collapse = "")),
                     label = "tab:time",
                     caption = "Estimation time [minutes] by optimization method"),
      type = "latex",
      include.rownames = TRUE,
      hline.after = c(-1, 0, nrow(tab_comp_time)),
      comment = FALSE,
      caption.placement = "top")

rm(tab_comp_time)

```

As a next step, we compare the parameter estimates from the "Nelder-Mead", "BFGS" and "hjn" algorithms with "zero" initial values. The "Nelder-Mead" and "BFGS" estimates differ clearly in terms of the split between the intercept and the coefficient of the Oxford Hip Score, the probabilities of component membership and the standard deviations (table \@ref(tab:tab-comp-coef)). The solution of the "hjn" method is rather extreme with no effect of the Oxford Hip Score in component 1, a standard deviation of almost 0 in component 1 and a very low probability of membership of component 1.

```{r tab-comp-coef, echo = FALSE, results = "asis"}

load("tab_comp_coef.RData")

load("tab_sum_mod1.RData")

print(xtable::xtable(tab_comp_coef, 
                     align = "lllrrr",
                     label = "tab:tab-comp-coef",
                     caption = 'Regression results of model1 with zero starting 
                     values in "Nelder-Mead", "BFGS" and "hjn" algorithms'),
      type = "latex",
      include.rownames = FALSE,
      hline.after = tab_sum_mod1$lindex,
      comment = FALSE,
      caption.placement = "top")

rm(tab_comp_coef, tab_sum_mod1)

```

To get a better understanding of the differences between the results of the "Nelder-Mead", "BFGS" and "hjn" algorithms we plot the densities of each component weighted by the probability of component membership.

```{r fig-comp-dens1_show, echo = TRUE, eval = FALSE}

nsim <- 100
hr <- 3.825244 # Population average Oxford Hip Score

# Nelder-Mead parameter estimates
n1    <- nsim*exp(3.8489)/(1 + exp(3.8489))
mean1 <- -0.0575 + 0.2233 * hr
sd1   <- exp(-1.8381)
n2    <- nsim*(1 - exp(3.8489)/(1 + exp(3.8489)))
mean2 <- 4.4022 + -0.9974 * hr
sd2   <- exp(0.1250)

# Make plot
ggplot2::ggplot(data = data.frame(x = c(-1, 1)), aes(x)) +
  ggplot2::stat_function(fun = dnorm, 
                         n = n1, 
                         args = list(mean = mean1, sd = sd1)) +
  ggplot2::stat_function(fun = dnorm, 
                         n = n2, 
                         args = list(mean = mean2, sd = sd2))

```

The densities in the solution of the "Nelder-Mead" method resemble the bi-model distribution observed in the data (figure \@ref(fig:fig-comp-dens1)). The densities in the solution of the "BFGS" method include two distributions with similar means but different standard deviations (figure \@ref(fig:fig-comp-dens2)). The densities in the solution of the "hjn" method include two distributions with similar means, but the distribution in component 1 shows low density mass due to the low probability of membership to component 1 (figure \@ref(fig:fig-comp-dens3)). The density plots also suggest that the model fit benefits more from improving the modeling of the distribution at the more frequently observed higher utilities rather than replicating the bi-modal distribution observed in the data. Overall, the differences between optimization methods show that it is very difficult to fit a two-component model to the data. We suspect that a simple one-component model would be more likely to converge towards a global optimum and would fit the data similarly well as the two-component model. 

```{r fig-comp-dens1, echo = FALSE, results = "asis", fig.height = 2.9, fig.width = 6,  fig.cap = 'Densities in components based on "Nelder-Mead" parameter estimates (observation with population average Oxford Hip Score 3.8489)'}

nsim <- 100
hr <- 3.825244 # Population average Oxford Hip Score

n1    <- nsim*exp(3.8489)/(1 + exp(3.8489))
mean1 <- -0.0575 + 0.2233 * hr
sd1   <- exp(-1.8381)
n2    <- nsim*(1 - exp(3.8489)/(1 + exp(3.8489)))
mean2 <- 4.4022 + -0.9974 * hr
sd2   <- exp(0.1250)

ggplot2::ggplot(data = data.frame(x = c(-1, 1)), aes(x)) +
  ggplot2::stat_function(fun = dnorm, 
                         n = n1, 
                         args = list(mean = mean1, sd = sd1)) +
  ggplot2::stat_function(fun = dnorm, 
                         n = n2, 
                         args = list(mean = mean2, sd = sd2)) +
  xlab("EQ-5D-3L utilities") +
  ylab("Density") +
  ggplot2::geom_vline(xintercept = c(-0.594, 0.883, 1), 
                      linetype = "dashed", 
                      colour = "gray") +
  ggplot2::coord_cartesian(ylim=c(0, 5), xlim = c(-1, 1)) +
  ggplot_theme

rm(n1, mean1, sd1, n2, mean2, sd2, nsim)

```

```{r fig-comp-dens2, echo = FALSE, results = "asis", fig.height = 2.9, fig.width = 6, fig.cap = 'Densities in components based on "BFGS" parameter estimates (observation with population average Oxford Hip Score 3.8489)'}

nsim <- 100
hr <- 3.825244 # Population average Oxford Hip Score

n1    <- nsim*exp(0.7283)/(1 + exp(0.7283))
mean1 <- 0.2358 + 0.1459 * hr
sd1   <- exp(-2.4623)
n2    <- nsim*(1 - exp(0.7283)/(1 + exp(0.7283)))
mean2 <- -0.4306 + 0.3135 * hr
sd2   <- exp(-1.2480)

ggplot2::ggplot(data = data.frame(x = c(-1, 1)), aes(x)) +
  ggplot2::stat_function(fun = dnorm, 
                         n = n1, 
                         args = list(mean = mean1, sd = sd1)) +
  ggplot2::stat_function(fun = dnorm, 
                         n = n2, 
                         args = list(mean = mean2, sd = sd2)) +
  xlab("EQ-5D-3L utilities") +
  ylab("Density") +
  ggplot2::geom_vline(xintercept = c(-0.594, 0.883, 1), 
                      linetype = "dashed", 
                      colour = "gray") +
  ggplot2::coord_cartesian(ylim=c(0, 5), xlim = c(-1, 1)) +
  ggplot_theme

rm(n1, mean1, sd1, n2, mean2, sd2, nsim)

```

```{r fig-comp-dens3, echo = FALSE, results = "asis", fig.height = 2.9, fig.width = 6, fig.cap = 'Densities in components based on "hjn" parameter estimates (observation with population average Oxford Hip Score 3.8489)'}

nsim <- 100

n1    <- nsim*exp(-2.1900)/(1 + exp(-2.1900))
mean1 <- 0.6910 + 0 * 3.825244
sd1   <- exp(-36.7370)
n2    <- nsim*(1 - exp(-2.1900))/(1 + exp(-2.1900))
mean2 <- -0.1490 + 0.2500 * 3.825244
sd2   <- exp(-1.5890)

ggplot2::ggplot(data = data.frame(x = c(-1, 1)), aes(x)) +
  ggplot2::stat_function(fun = dnorm, 
                         n = n1, 
                         args = list(mean = mean1, sd = sd1)) +
  ggplot2::stat_function(fun = dnorm, 
                         n = n2, 
                         args = list(mean = mean2, sd = sd2)) +
  xlab("EQ-5D-3L utilities") +
  ylab("Density") +
  ggplot2::geom_vline(xintercept = c(-0.594, 0.883, 1), 
                      linetype = "dashed", 
                      colour = "gray") +
  ggplot2::coord_cartesian(ylim=c(0, 5), xlim = c(-1, 1)) +
  ggplot_theme

rm(n1, mean1, sd1, n2, mean2, sd2, nsim)

```

The differences in parameter estimates from different optimization methods show that the choice of the optimization algorithm and initial values is very important for parameter identification. As adjusted limited dependent variable mixture models are frequently used for tasks that rely on predictions, we also compare expected values from the "Nelder-Mead" and "BFGS" methods to the expected values from the "hjn" method. Expected values from the "Nelder-Mead" and "BFGS" methods differed from expected values from the "hjn" method in particular for observations with lower expected values (figure \@ref(fig:plot-comp-pred)). Interestingly, the expected values from the "Nelder-Mead" method deviate less from the expected values from the "hjn" method than the expected values from the "BFGS" method although the "BFGS" method yielded a higher log-likelihood in the optimum, and the "BFGS" results are mixtures of densities with higher means similar to the solution of the "hjn" algorithm.

```{r plot-comp-pred, echo = FALSE, results = "asis", fig.height = 2.9, fig.width = 6, fig.cap = 'Expected values from model 1, "Nelder-Mead" and "BFGS" versus "hjn" with zero starting values'}

load("plot_comp_pred.RData")

plot_comp_pred

rm(plot_comp_pred)

```

An visual inspection of mean residuals over deciles of expected values shows that model 1 fits the data poorly, and that the patterns of over- and under-predictions of observed values are similar across optimization methods with different log-likelihoods (figure \@ref(fig:plot-comp-mhl1), figure \@ref(fig:plot-comp-mhl2) and figure \@ref(fig:plot-comp-mhl3) in the appendix).

### Constrained optimization with user-defined initial values

We can also fit model 1 with user-defined starting values and box constraints. When constraints are imposed the function aldvmm::aldvmm() uses the optimization method "L-BFGS-B". We set the initial value of the intercept of the probability of belonging to component 1 to 0.7283 as in the solution of the "BFGS" optimization method with "zero" starting values (table \@ref(tab:tab-comp-coef)). We impose a lower limit of -3 to the log-standard deviations in both components. The function aldvmm::aldvmm() returns a warning that the covariance matrix included negative values on the diagonal. We see that these values are the variances of the intercept and the log-standard deviation in component 2 (table \@ref(tab:tab-sum-cstr)). The log-likelihood amounts to -627.78, which is lower than the log-likelihood in the solution of the "Nelder-Mead" optimization method with "zero" starting values. The parameter estimates do not resemble any of the solutions of the unconstrained "Nelder-Mead", "BFGS" or "hjn" optimization methods with "zero" values (table \@ref(tab:tab-comp-coef)), which further emphasizes the difficulties in finding a global optimum of the likelihood with English EQ-5D-3L utilities after hip replacement.

```{r tab-cstr-show, echo = TRUE, eval = FALSE}
init <- c(0,    0,   0,   0,    0,    0,    0.7283)
lo   <- c(-Inf, -Inf, -3,  -Inf, -Inf, -3, -Inf)
hi   <- c(Inf,  Inf,  Inf, Inf,  Inf,  Inf,  Inf)

fit1_cstr <- aldvmm::aldvmm(data = df,
                            formula = formula,
                            psi = c(0.883, -0.594),
                            ncmp = 2,
                            init.est = init,
                            init.lo = lo,
                            init.hi = hi)

summary(fit1_cstr)
```

```{r tab-sum-cstr, echo = FALSE, results = "asis"}

load("tab_sum_cstr.RData")

print(xtable::xtable(tab_sum_cstr$table, 
                     align = "lllrrrrrr",
                     label = "tab:tab-sum-cstr",
                     caption = 'Regression results of model1 with "hjn" optimization method and constrained user-defined starting values'),
      type = "latex",
      include.rownames = FALSE,
      hline.after = tab_sum_cstr$lindex,
      comment = FALSE,
      caption.placement = "top")

rm(tab_sum_cstr)

```

### Single-component model

As the solution of the "hjn" algorithm included a component with very low probability, we also estimate a single-component model.

```{r tobit-fit, echo = TRUE, eval = FALSE}
fit <- aldvmm::aldvmm(data = df,
                      formula = eq5d ~ hr | 1,
                      psi = c(0.883, -0.594),
                      ncmp = 1,
                      init.method = "zero",
                      optim.method = "hjn")

summary(fit)
```

The coefficients of the single-component model are relatively similar to the solution of the "hjn" algorithm for the two-component model (table \@ref(tab:tab-sum-tobit)). The Akaike information criterion amounts to 1'275.61 which is larger than the values of the "BFGS" (-1'398.65) and "hjn" (-66'100.87) solutions of the two-component model and thus suggests worse fit.

```{r tab-sum-tobit, echo = FALSE, results = "asis"}

load("tab_sum_tobit.RData")

print(xtable::xtable(tab_sum_tobit$table, 
                     align = "lllrrrrrr",
                     label = "tab:tab-sum-tobit",
                     caption = 'Regression results of model1 with 1 component, zero starting 
                     values in "hjn" algorithm'),
      type = "latex",
      include.rownames = FALSE,
      hline.after = tab_sum_tobit$lindex,
      comment = FALSE,
      caption.placement = "top")

rm(tab_sum_tobit)

```

## Model 2

Because model 1 is difficult to fit and shows poor fit to the data, we also estimate model 2 with a coefficient of the Oxford Hip score in the multinomial logit model of component membership. For this fit, we use the method "hjn" with estimates from @HernandezAlava2015 as starting values.

```{r model2-fit, echo = TRUE, eval = FALSE}
init <- c(-.40293118, .30502755, .22614716, .14801581, -.70755741, 0, 
          -1.2632051, -2.4541401)

fit2 <- aldvmm::aldvmm(data = df,
                       formula = eq5d ~ hr | hr,
                       psi = c(0.883, -0.594),
                       ncmp = 2,
                       init.est = init,
                       optim.method = "hjn")

summary(fit2)

```

The log-likelihood of model 2 in the "hjn" optimum is larger than the log-likelihood of model 1 in the "hjn" optimum due to the extra parameter (table \@ref(tab:tab-sum-mod2)). However, the Akaike information criterion of model 2 (-1'866.97) is larger than the Akaike information criterion of model 1 (-66'100.87), which suggests that the increase in the log-likelihood does not justify the extra parameter in the multinomial logit part.\footnote{In the aldvmm::aldvmm() output, smaller values of the Akaike information criterion indicate better goodness of fit.} 

```{r tab-sum-mod2, echo = FALSE, results = "asis"}

load("tab_sum_mod2.RData")

print(xtable::xtable(tab_sum_mod2$table, 
                     align = "lllrrrrrr",
                     label = "tab:tab-sum-mod2",
                     caption = 'Regression results of model 2 with user-defined 
                     starting values in the "hjn" algorithm'),
      type = "latex",
      include.rownames = FALSE,
      hline.after = tab_sum_mod2$lindex,
      comment = FALSE,
      caption.placement = "top")

rm(tab_sum_mod2)

```

## Comparison to results by @HernandezAlava2015

To validate the R implementation of adjusted limited dependent variable models we compare the results of three models to the results from the STATA\textsuperscript{\textregistered} ALDVMM package published in @HernandezAlava2015.

- Model 1 with default optimization settings.

- Model 1 initial values from constant-only model.

- Model 2 with user-defined initial values.

Because we draw a random sub-sample of the data, we do not expect identical results, but similar findings would be expected if the model is implemented in a similar manner as in STATA\textsuperscript{\textregistered}.

We use the results of the "BFGS" method with "zero" starting values as default estimates of model 1, as this solution is found most frequently by different optimization methods (table \@ref(tab:ll)), and the results of the "hjn" method with "zero" starting values are rather extreme and difficult to interpret.

```{r tab-comp-stata-show, echo = TRUE, eval = FALSE}
# Model 1 with default optimization settings
fit1_default <- aldvmm::aldvmm(data = df,
                               formula = eq5d ~ hr | 1,
                               psi = c(0.883, -0.594),
                               ncmp = 2,
                               init.method = "zero",
                               optim.method = "BFGS")

# Model 1 initial values from constant-only model
fit1_const <- aldvmm::aldvmm(data = df,
                             formula = eq5d ~ hr | 1,
                             psi = c(0.883, -0.594),
                             ncmp = 2,
                             init.method = "constant",
                             optim.method = "hjn")

# Model 2 with user-defined initial values.
init <- c(-.40293118, .30502755, .22614716, .14801581, -.70755741, 0, 
          -1.2632051, -2.4541401)

fit2 <- aldvmm::aldvmm(data = df,
                       formula = eq5d ~ hr | hr,
                       psi = c(0.883, -0.594),
                       ncmp = 2,
                       init.est = init,
                       optim.method = "hjn")
```

The results of model 1 from the "BFGS" method with "zero" starting values differed considerably from the default estimates reported by @HernandezAlava2015 (table \@ref(tab:tab-comp-stata)). However, the default estimates by @HernandezAlava2015 include extreme estimates without standard errors in component 2 (table \@ref(tab:stata1) in the appendix). Our estimates of parameters, standard errors and the log-likelihood of model 1 with initial values from a constant-only model are very similar to the results from @HernandezAlava2015 (tables \@ref(tab:stata2) and \@ref(tab:tab-sum-const) in the appendix). The parameter estimates of model 2 with user-defined starting values are somewhat similar, and the standard errors of parameters with similar point estimates are fairly consistent (tables \@ref(tab:stata3) and \@ref(tab:tab-sum-mod2stata) in the appendix). Differences in the analyzed sub-samples could be a possible reason for the observed discrepancies. We do not deem these findings indications of errors in the 'aldvmm' implementation of adjusted limited dependent variable mixture models.

```{r tab-calc-stata, echo = FALSE, results = "asis"}

load("tab_sum_mod1bfgs.RData")
load("tab_sum_const.RData")
load("tab_sum_mod2stata.RData")
load("tab_sum_stata1.RData")
load("tab_sum_stata2.RData")
load("tab_sum_stata3.RData")

ctab <- matrix(NA, 
               nrow = nrow(tab_sum_mod2stata$table),
               ncol = 8)

ctab[, 1:2] <- as.matrix(tab_sum_mod2stata$table[, 1:2])
ctab[nrow(ctab), 2] <- ""

tabvec <- c("tab_sum_mod1bfgs",  "tab_sum_stata1", 
            "tab_sum_const",     "tab_sum_stata2", 
            "tab_sum_mod2stata", "tab_sum_stata3")

for (i in tabvec){
  nr <- nrow(get(i)$table)
  for (j in 1:(nr - 1)){
    ctab[j, 2 + match(i, tabvec)] <- as.matrix(get(i)$table)[j, 3]
    ctab[nrow(ctab), 2 + match(i, tabvec)] <- as.matrix(get(i)$table)[nr, 2]
  }
  rm(nr)
}
rm(i, j, tabvec)

ctab <- rbind(c("", "", "default", "", "constant", "", "model 2", ""),
              c("", "", "R", "STATA", "R", "STATA", "R", "STATA"),
              ctab)

lindex <- tab_sum_mod2stata$lindex + 2
lindex[1] <- -1

print(xtable::xtable(ctab, 
                     align = "lllrrrrrr",
                     label = "tab:tab-comp-stata",
                     caption = "Comparison of point estimates of parameters in R and STATA\textsuperscript{\textregistered}"),
      type = "latex",
      include.colnames = FALSE,
      include.rownames = FALSE,
      hline.after = lindex,
      comment = FALSE,
      caption.placement = "top")

rm(list = ls(pattern = "tab_sum"))
rm(ctab, lindex)
```

# Discussion

Adjusted limited dependent variable mixture models are powerful tools for regression analysis of health state utility data. Unlike standard regression models, adjusted limited dependent variable mixture models account for limits, gaps and multi-modal distributions.

The comparison of different optimization methods in EQ-5D-3L utility data from English patients after hip replacement in 2011 and 2012 [@NHSDigital2013] shows that the likelihood function can be challenging to maximize and can converge at extreme solutions. The package 'aldvmm' offers a variety of optimization algorithms and methods for generating initial values which is an important strength in such challenging situations. It is essential to assess different optimization algorithms and methods for initial values before interpreting the parameter estimates or predictions of adjusted limited dependent variable mixture models. 

The analysis of the EQ-5D-3L utility data also suggests that simpler models with fewer components should be considered when multi-component models are difficult to fit. Even in single-component models, the adjusted limited dependent variable mixture model accounts for limits and gaps which can improve fit compared to traditional regression techniques.

Although coefficients of expected values can be interpreted as marginal effects within each component, they cannot interpreted in terms of overall expected values. Thus, average marginal effects and average treatment effects need to be calculated from predictions using the generic function predict(). Standard errors of marginal effects or average treatment effects can be calculated using the standard errors of fitted values for observed and counterfactual covariate values. 

In situations with repeated utility measures, the package 'aldvmm' only allows fixed effect estimations with individual/group and time fixed effects. This might be an important limitation in the analysis of clinical trial data when the statistical analysis plan of the clinical study specifies models with individual/group and time random effects. However, causal inference on treatment effects with repeated measures requires an adjustment of treatment effects for general time trends when there is dynamic selection in the population, e.g. because treated individuals survive longer and thus are over-represented in later measurements. The researcher who aims to make causal inference using repeated measures faces a trade-off between the efficiency of the random effects model and the causal interpretation of treatment effects in a fixed-effects model. It is recommended to assess the uncorrelatedness of random effects with the treatment using the Hausman test in a generalized linear model before making an informed modeling decision. 

Mixed effects implementations of 'aldvmm' would be an important improvement over the current implementation. However, a mixed effects implementation would require other optimization techniques than the package 'aldvmm'. A Bayesian implementation of the likelihood from @HernandezAlava2015 would be a pragmatic and powerful approach to introduce random effects to any parameter in the model. Obviously, Bayesian models would also provide relevant information about the full posterior distributions, but the definition of informative priors can be challenging with the fairly complex adjusted limited dependent variable mixture models.

Possible extensions of 'aldvmm' could include the following features.

- Adjusted limited dependent variable beta mixture models [@Gray2018b].

- Mixed model implementation for repeated measures.

- Generic function for the calculation of average marginal effects and their standard errors.

Any suggestions are welcome.

\newpage

# References

<div id="refs"></div>

\newpage
# Appendix

## Covariance matrices across optimization methods

Covariance matrices were incomplete or missing entirely (`FALSE`) in multiple optimization approaches (table \@ref(tab:cov))

```{r tab-comp-cov, echo = FALSE, results = 'asis'}

load("tab_comp_cov.RData")

print(xtable::xtable(tab_comp_cov, 
                     align = paste0("l", 
                                    paste(rep("r", ncol(tab_comp_cov)), 
                                          collapse = "")),
                     label = "tab:cov",
                     caption = "Covariance matrix by optimization method"),
      type = "latex",
      include.rownames = TRUE,
      hline.after = c(-1, 0, nrow(tab_comp_cov)),
      comment = FALSE,
      caption.placement = "top")

rm(tab_comp_cov)

```

\newpage

## Modified Hosmer-Lemeshow test

```{r mhl-show, echo = TRUE, eval = FALSE}
# Number of percentiles  
ngroup <- 10

# Extract expected values and residuals
yhat <- fit1_all[["zero"]][["Nelder-Mead"]][["pred"]][["yhat"]]
res <- fit1_all[["zero"]][["Nelder-Mead"]][["pred"]][["res"]]

# Make groups
group <- as.numeric(cut(yhat, breaks = ngroup), na.rm=TRUE)

# Auxiliary regression
aux <- stats::lm(res ~ factor(group))

# Data set of predictions from auxiliary regressions
newdf <- data.frame(group = unique(group)[order(unique(group))])
predict <- predict(aux, 
                   newdata = newdf, 
                   se.fit = TRUE, 
                   interval = 'confidence', 
                   level = 0.95)

plotdat <- as.data.frame(rbind(
  cbind(group = newdf$group, 
        outcome = "mean",
        value = predict$fit[ , 'fit']),
  cbind(group = newdf$group, 
        outcome = "ll",
        value = predict$fit[ , 'lwr']),
  cbind(group = newdf$group, 
        outcome = "ul",
        value = predict$fit[ , 'upr'])
))

# Make plot
plot <- ggplot2::ggplot(plotdat, aes(x = factor(as.numeric(group)), 
                                     y = as.numeric(value), 
                                     group = factor(outcome))) +
  geom_line(aes(linetype = factor(outcome)))
```  
  
```{r plot-comp-mhl1, echo = FALSE, results = "asis", fig.height = 2.5, fig.width = 6, fig.cap = 'Mean residuals over deciles of expected values, "Nelder-Mead" with "zero" starting values'}

load("plot_comp_mhl1.RData")

plot_comp_mhl1

rm(plot_comp_mhl1)

```

```{r plot-comp-mhl2, echo = FALSE, results = "asis", fig.height = 2.5, fig.width = 6, fig.cap = 'Mean residuals over deciles of expected values, "BFGS" with "zero" starting values'}

load("plot_comp_mhl2.RData")

plot_comp_mhl2

rm(plot_comp_mhl2)

```

```{r plot-comp-mhl3, echo = FALSE, results = "asis", fig.height = 2.5, fig.width = 6, fig.cap = 'Mean residuals over deciles of expected values, "hjn" with "zero" starting values'}

load("plot_comp_mhl3.RData")

plot_comp_mhl3

rm(plot_comp_mhl3)

```

\newpage
## Comparison to results by @HernandezAlava2015

```{r tab-sum-stata1, echo = FALSE, results = "asis"}

load("tab_sum_stata1.RData")

print(xtable::xtable(tab_sum_stata1$table, 
                     align = "lllrrrrrr",
                     label = "tab:stata1",
                     caption = "Regression results from Hernandez Alava and Wailoo 2015, model 1 with default initial values from function ml"),
      type = "latex",
      include.rownames = FALSE,
      hline.after = tab_sum_stata1$lindex,
      comment = FALSE,
      caption.placement = "top")

rm(tab_sum_stata1)

```

```{r tab-sum-const, echo = FALSE, results = "asis"}

load("tab_sum_const.RData")

print(xtable::xtable(tab_sum_const$table, 
                     align = "lllrrrrrr",
                     label = "tab:tab-sum-const",
                     caption = 'Regression results of model 2 with user-defined 
                     starting values in the "hjn" algorithm'),
      type = "latex",
      include.rownames = FALSE,
      hline.after = tab_sum_const$lindex,
      comment = FALSE,
      caption.placement = "top")

rm(tab_sum_const)

```


```{r tab-sum-stata2, echo = FALSE, results = "asis"}

load("tab_sum_stata2.RData")

print(xtable::xtable(tab_sum_stata2$table, 
                     align = "lllrrrrrr",
                     label = "tab:stata2",
                     caption = "Regression results from Hernandez Alava and Wailoo 2015, model 1 with initial values from a constant-only model"),
      type = "latex",
      include.rownames = FALSE,
      hline.after = tab_sum_stata2$lindex,
      comment = FALSE,
      caption.placement = "top")

rm(tab_sum_stata2)

```

```{r tab-sum-mod2stata, echo = FALSE, results = "asis"}

load("tab_sum_mod2stata.RData")

print(xtable::xtable(tab_sum_mod2stata$table, 
                     align = "lllrrrrrr",
                     label = "tab:tab-sum-mod2stata",
                     caption = 'Regression results of model 2 with user-defined 
                     starting values in the "hjn" algorithm'),
      type = "latex",
      include.rownames = FALSE,
      hline.after = tab_sum_mod2stata$lindex,
      comment = FALSE,
      caption.placement = "top")

rm(tab_sum_mod2stata)

```

```{r tab-sum-stata3, echo = FALSE, results = "asis"}

load("tab_sum_stata3.RData")

print(xtable::xtable(tab_sum_stata3$table, 
                     align = "lllrrrrrr",
                     label = "tab:stata3",
                     caption = "Regression results from Hernandez Alava and Wailoo 2015, model 2 with user-defined initial values"),
      type = "latex",
      include.rownames = FALSE,
      hline.after = tab_sum_stata3$lindex,
      comment = FALSE,
      caption.placement = "top")

rm(tab_sum_stata3)

```

```{r end, echo = FALSE, results = "hide"}

rm(list = ls())

```
